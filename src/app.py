"""
Application Gradio pour l'extraction de connaissances de plantes agricoles.
Interface web pour analyser les images, visualiser les r√©sultats et exporter les donn√©es.
"""

import gradio as gr
from pathlib import Path
from typing import Tuple, Optional, Dict, List
import time
import os
import shutil
from glob import glob

# Imports des modules du projet
from src.config import config
from src.ontology_loader import OntologyLoader
from src.llm_extractor import LLMExtractor
from src.similarity_calculator import SimilarityCalculator
from src.ontology_matcher import OntologyMatcher
from src.datavault_generator import DataVaultGenerator, DataVaultSchema
from src.exporters.json_exporter import JSONExporter
from src.exporters.rdf_exporter import RDFExporter
from src.exporters.sql_exporter import SQLExporter


# ========== INITIALISATION GLOBALE ==========

print("\n" + "=" * 60)
print("üåæ SYST√àME D'EXTRACTION DE CONNAISSANCES DE PLANTES")
print("=" * 60)

# Valider les chemins
if not config.validate_paths():
    print("‚ùå Erreur: Fichiers requis manquants. V√©rifiez la configuration.")
    exit(1)

config.display_config()

# Charger l'ontologie par d√©faut (sera recharg√©e dynamiquement selon la s√©lection)
print("\nüìö Chargement de l'ontologie...")
ontology = OntologyLoader(config.ONTOLOGY_PATH)
ontology_stats = ontology.get_statistics()
print(f"‚úÖ Ontologie charg√©e avec succ√®s")

# Initialiser le calculateur de similarit√© par d√©faut (sera recr√©√© avec les param√®tres s√©lectionn√©s)
print("\nüß† Initialisation du calculateur de similarit√©...")
similarity_calc = SimilarityCalculator(
    embedding_model=config.EMBEDDING_MODEL,
    ollama_base_url=config.OLLAMA_BASE_URL,
    algorithm=config.SIMILARITY_ALGORITHM
)

# Initialiser l'extracteur LLM
print("\nü§ñ Connexion √† Ollama...")
try:
    llm_extractor = LLMExtractor(config.OLLAMA_BASE_URL, config.LLAVA_MODEL)
    if not llm_extractor.check_model_availability():
        print("‚ö†Ô∏è  Le mod√®le LLaVA n'est pas disponible. Certaines fonctionnalit√©s seront limit√©es.")
except Exception as e:
    print(f"‚ö†Ô∏è  Impossible de se connecter √† Ollama: {e}")
    llm_extractor = None

# Variables globales pour stocker les sch√©mas g√©n√©r√©s
last_schema: Optional[DataVaultSchema] = None
batch_schemas: Dict[str, DataVaultSchema] = {}


# ========== FONCTIONS DE L'INTERFACE ==========

def clear_outputs():
    """Affiche un message de chargement avant l'analyse."""
    status_msg = "‚è≥ **ANALYSE EN COURS**\n\nVeuillez patienter, le mod√®le traite votre image..."
    return status_msg, "", "", ""


def analyze_image(
    image,
    selected_model: str,
    selected_ontology: str,
    selected_similarity: str,
    threshold_entities: float,
    threshold_relations: float,
    threshold_attributes: float,
    progress=gr.Progress()
) -> Tuple[str, str, str, str]:
    """
    Analyse une image et retourne les r√©sultats organis√©s par Hub.

    Args:
        image: Image upload√©e
        selected_model: Mod√®le LLM √† utiliser
        selected_ontology: Fichier d'ontologie √† utiliser
        selected_similarity: Algorithme de similarit√© √† utiliser
        threshold_entities: Seuil pour entit√©s
        threshold_relations: Seuil pour relations
        threshold_attributes: Seuil pour attributs
        progress: Gestionnaire de progression Gradio

    Returns:
        Tuple (status_msg, lemmes_str, results_by_hub_str, stats_str)
    """
    global last_schema

    # Initialiser la progression imm√©diatement
    progress(0, desc="D√©marrage de l'analyse...")

    if image is None:
        return "‚ùå Aucune image fournie", "", "", ""

    try:
        start_time = time.time()

        # √âTAPE 1: Extraction de lemmes avec le mod√®le s√©lectionn√©
        progress(0.05, desc="Connexion au mod√®le LLM...")
        print(f"\nüîç Analyse de l'image avec le mod√®le {selected_model}...")
        current_extractor = LLMExtractor(config.OLLAMA_BASE_URL, selected_model)

        # V√©rifier la disponibilit√© (mais continuer m√™me si la v√©rification √©choue)
        progress(0.1, desc="V√©rification du mod√®le...")
        is_available = current_extractor.check_model_availability()
        if not is_available:
            print(f"‚ö†Ô∏è  La v√©rification du mod√®le a √©chou√©, mais on essaie quand m√™me...")

        progress(0.2, desc="Extraction des lemmes...")
        lemmas = current_extractor.extract_lemmas(image)

        if not lemmas:
            return "‚ö†Ô∏è  Aucun lemme extrait", "", "", ""

        progress(0.4, desc=f"{len(lemmas)} lemmes extraits, chargement ontologie...")

        # √âTAPE 2: Charger l'ontologie s√©lectionn√©e
        available_ontologies = config.get_available_ontologies()
        ontology_path = available_ontologies.get(selected_ontology, config.ONTOLOGY_PATH)
        current_ontology = OntologyLoader(ontology_path)
        print(f"üìö Ontologie charg√©e: {selected_ontology}")

        # √âTAPE 3: Initialiser le calculateur de similarit√© avec l'algorithme s√©lectionn√©
        progress(0.45, desc="Initialisation du calculateur de similarit√©...")
        current_similarity_calc = SimilarityCalculator(
            embedding_model=config.EMBEDDING_MODEL,
            ollama_base_url=config.OLLAMA_BASE_URL,
            algorithm=selected_similarity
        )
        print(f"üìä Algorithme de similarit√©: {selected_similarity}")

        # √âTAPE 4: Classification ontologique
        progress(0.5, desc="Classification ontologique...")
        thresholds = {
            "entities": threshold_entities,
            "relations": threshold_relations,
            "attributes": threshold_attributes
        }

        matcher = OntologyMatcher(current_ontology, current_similarity_calc, thresholds)
        hubs, links, satellites = matcher.classify_lemmas(lemmas, "uploaded_image")

        progress(0.8, desc="G√©n√©ration du sch√©ma Data Vault...")

        # √âTAPE 5: G√©n√©ration sch√©ma Data Vault
        generator = DataVaultGenerator()
        schema = generator.generate_schema(hubs, links, satellites, "uploaded_image", lemmas)

        # Valider le sch√©ma
        progress(0.9, desc="Validation du sch√©ma...")
        errors = generator.validate_schema(schema)

        # Stocker le sch√©ma pour l'export
        last_schema = schema

        progress(0.95, desc="Formatage des r√©sultats...")

        # Pr√©parer l'affichage des lemmes
        lemmas_str = f"### Lemmes extraits ({len(lemmas)})\n\n"
        lemmas_str += "```\n" + ", ".join(lemmas) + "\n```"

        # Organiser les r√©sultats par Hub
        results_by_hub_str = _format_results_by_hub(hubs, links, satellites)

        # Statistiques
        stats = schema.get_statistics()
        elapsed_time = time.time() - start_time

        # Mapper les valeurs pour l'affichage
        similarity_names = {
            "hybrid": "Hybride (Jaro-Winkler + Embeddings)",
            "lexical": "Lexical (Jaro-Winkler)",
            "semantic": "S√©mantique (Embeddings)"
        }

        stats_str = f"""
### Statistiques

**Configuration**
- Mod√®le: {selected_model}
- Ontologie: {selected_ontology}
- Similarit√©: {similarity_names.get(selected_similarity, selected_similarity)}
- Temps: {elapsed_time:.2f}s

**√âl√©ments d√©tect√©s**
- Hubs (entit√©s): {stats["total_hubs"]}
- Links (relations): {stats["total_links"]}
- Satellites (attributs): {stats["total_satellites"]}

**Confiance moyenne**
- Hubs: {stats['average_confidence']['hubs']:.2%}
- Links: {stats['average_confidence']['links']:.2%}
- Satellites: {stats['average_confidence']['satellites']:.2%}

"""

        #if errors:
        #    stats_str += f"\n\n**Avertissements** ({len(errors)})\n" + "\n".join([f"- {e}" for e in errors[:5]])

        progress(1.0, desc="Analyse termin√©e")

        return "‚úÖ **Analyse termin√©e avec succ√®s**", lemmas_str, results_by_hub_str, stats_str

    except Exception as e:
        progress(1.0, desc="Erreur lors de l'analyse")
        return f"‚ùå Erreur lors de l'analyse: {str(e)}", "", "", ""


def process_multiple_images(
    uploaded_files: List,
    selected_model: str,
    selected_ontology: str,
    selected_similarity: str,
    threshold_entities: float,
    threshold_relations: float,
    threshold_attributes: float,
    progress=gr.Progress()
) -> Tuple[str, str]:
    """
    Traite plusieurs images upload√©es.

    Args:
        uploaded_files: Liste des fichiers images upload√©s
        selected_model: Mod√®le LLM √† utiliser
        selected_ontology: Fichier d'ontologie √† utiliser
        selected_similarity: Algorithme de similarit√© √† utiliser
        threshold_entities: Seuil pour entit√©s
        threshold_relations: Seuil pour relations
        threshold_attributes: Seuil pour attributs
        progress: Gestionnaire de progression Gradio

    Returns:
        Tuple (status_msg, results_summary)
    """
    global batch_schemas

    # Initialiser la progression imm√©diatement
    progress(0, desc="D√©marrage du traitement par lot...")

    if not uploaded_files:
        return "‚ùå Aucun fichier upload√©", ""

    try:
        # Convertir en liste si un seul fichier
        if not isinstance(uploaded_files, list):
            uploaded_files = [uploaded_files]

        # Filtrer les fichiers None
        image_files = [f for f in uploaded_files if f is not None]

        if not image_files:
            return "‚ùå Aucune image valide upload√©e", ""

        total_images = len(image_files)
        print(f"\nüìÅ Traitement de {total_images} images upload√©es")

        progress(0.05, desc=f"Pr√©paration pour traiter {total_images} images...")

        # Charger l'ontologie une seule fois
        progress(0.1, desc="Chargement de l'ontologie...")
        available_ontologies = config.get_available_ontologies()
        ontology_path = available_ontologies.get(selected_ontology, config.ONTOLOGY_PATH)
        current_ontology = OntologyLoader(ontology_path)

        # Initialiser le calculateur de similarit√© une seule fois
        progress(0.15, desc="Initialisation du calculateur de similarit√©...")
        current_similarity_calc = SimilarityCalculator(
            embedding_model=config.EMBEDDING_MODEL,
            ollama_base_url=config.OLLAMA_BASE_URL,
            algorithm=selected_similarity
        )

        # Initialiser l'extracteur LLM
        progress(0.2, desc="Connexion au mod√®le LLM...")
        current_extractor = LLMExtractor(config.OLLAMA_BASE_URL, selected_model)

        # Thresholds
        thresholds = {
            "entities": threshold_entities,
            "relations": threshold_relations,
            "attributes": threshold_attributes
        }

        # R√©initialiser les sch√©mas batch
        batch_schemas = {}

        # Variables pour statistiques globales
        total_hubs = 0
        total_links = 0
        total_satellites = 0
        successful_images = 0
        failed_images = 0

        # Dictionnaire pour stocker les lemmes de chaque image
        image_lemmas = {}

        start_time = time.time()

        # Traiter chaque image
        for idx, image_file in enumerate(image_files, 1):
            # Obtenir le chemin du fichier upload√©
            image_path = image_file if isinstance(image_file, str) else image_file.name
            image_name = os.path.basename(image_path)

            # Mise √† jour de la progression (de 0.2 √† 0.95)
            progress_base = 0.2 + ((idx - 1) / total_images) * 0.75
            progress(progress_base, desc=f"üì∏ Image {idx}/{total_images}: {image_name}")

            try:
                # Extraction de lemmes
                progress(progress_base + 0.1 * 0.75 / total_images, desc=f"üîç Extraction lemmes: {image_name}")
                lemmas = current_extractor.extract_lemmas(image_path)

                if not lemmas:
                    print(f"‚ö†Ô∏è  Aucun lemme extrait pour {image_name}")
                    failed_images += 1
                    image_lemmas[image_name] = []
                    continue

                # Stocker les lemmes
                image_lemmas[image_name] = lemmas
                print(f"‚úÖ {image_name}: {len(lemmas)} lemmes extraits")

                # Classification ontologique
                progress(progress_base + 0.3 * 0.75 / total_images, desc=f"üè∑Ô∏è  Classification: {image_name}")
                matcher = OntologyMatcher(current_ontology, current_similarity_calc, thresholds)
                hubs, links, satellites = matcher.classify_lemmas(lemmas, image_name)

                # G√©n√©ration sch√©ma Data Vault
                progress(progress_base + 0.6 * 0.75 / total_images, desc=f"üíæ G√©n√©ration sch√©ma: {image_name}")
                generator = DataVaultGenerator()
                schema = generator.generate_schema(hubs, links, satellites, image_name, lemmas)

                # Stocker le sch√©ma
                batch_schemas[image_name] = schema

                # Mettre √† jour les statistiques
                stats = schema.get_statistics()
                total_hubs += stats["total_hubs"]
                total_links += stats["total_links"]
                total_satellites += stats["total_satellites"]
                successful_images += 1

                print(f"‚úÖ {image_name}: {stats['total_hubs']} hubs, {stats['total_links']} links, {stats['total_satellites']} satellites")

            except Exception as e:
                print(f"‚ùå Erreur lors du traitement de {image_name}: {str(e)}")
                failed_images += 1
                image_lemmas[image_name] = []
                continue

        elapsed_time = time.time() - start_time
        progress(0.95, desc="G√©n√©ration du r√©sum√© des r√©sultats...")

        # (Le r√©sum√© sera g√©n√©r√© ci-dessous)

        progress(1.0, desc="Traitement termin√©")

        # Mapper les valeurs pour l'affichage
        similarity_names = {
            "hybrid": "Hybride (Jaro-Winkler + Embeddings)",
            "lexical": "Lexical (Jaro-Winkler)",
            "semantic": "S√©mantique (Embeddings)"
        }

        # G√©n√©rer le r√©sum√©
        results_summary = f"""
## R√©sultats du traitement par lot

### Configuration
- Nombre d'images: {total_images}
- Mod√®le: {selected_model}
- Ontologie: {selected_ontology}
- Similarit√©: {similarity_names.get(selected_similarity, selected_similarity)}

### Statistiques globales
- Images trait√©es avec succ√®s: {successful_images}/{total_images}
- Images en erreur: {failed_images}/{total_images}
- Temps total: {elapsed_time:.2f}s
- Temps moyen par image: {elapsed_time/total_images:.2f}s

### √âl√©ments d√©tect√©s (total)
- Hubs (entit√©s): {total_hubs}
- Links (relations): {total_links}
- Satellites (attributs): {total_satellites}

### D√©tails par image
"""

        # Ajouter les d√©tails de chaque image
        for image_name in image_lemmas.keys():
            lemmas = image_lemmas.get(image_name, [])

            results_summary += f"\n---\n\n### üì∏ {image_name}\n\n"

            # Afficher les lemmes
            if lemmas:
                results_summary += f"**Lemmes extraits ({len(lemmas)}):**\n\n"
                results_summary += "```\n" + ", ".join(lemmas) + "\n```\n\n"
            else:
                results_summary += "‚ö†Ô∏è Aucun lemme extrait\n\n"

            # Afficher les statistiques si le sch√©ma existe
            if image_name in batch_schemas:
                schema = batch_schemas[image_name]
                stats = schema.get_statistics()
                results_summary += f"""**R√©sultats Data Vault:**
- Hubs (entit√©s): {stats['total_hubs']}
- Links (relations): {stats['total_links']}
- Satellites (attributs): {stats['total_satellites']}
- Confiance moyenne: Hubs {stats['average_confidence']['hubs']:.2%}, Links {stats['average_confidence']['links']:.2%}, Satellites {stats['average_confidence']['satellites']:.2%}
"""
            else:
                results_summary += "‚ùå Traitement √©chou√©\n"

        status_msg = f"‚úÖ **Traitement termin√©**\n\n{successful_images} images trait√©es avec succ√®s sur {total_images}"

        return status_msg, results_summary

    except Exception as e:
        return f"‚ùå Erreur lors du traitement: {str(e)}", ""


def export_batch_results(export_format: str) -> Tuple[str, Optional[str]]:
    """
    Exporte tous les sch√©mas du traitement par lot.

    Args:
        export_format: Format d'export ("JSON", "RDF", "SQL")

    Returns:
        Tuple (message, chemin_fichier)
    """
    global batch_schemas

    if not batch_schemas:
        return "‚ùå Aucun sch√©ma √† exporter. Traitez d'abord un r√©pertoire d'images.", None

    try:
        export_dir = Path(config.EXPORT_PATH)
        export_dir.mkdir(parents=True, exist_ok=True)

        timestamp = time.strftime("%Y%m%d_%H%M%S")
        batch_dir = export_dir / f"batch_{timestamp}"
        batch_dir.mkdir(exist_ok=True)

        exported_count = 0

        for image_name, schema in batch_schemas.items():
            # Nettoyer le nom de fichier
            clean_name = Path(image_name).stem

            if export_format == "JSON":
                exporter = JSONExporter()
                output_path = batch_dir / f"{clean_name}.json"
                exporter.export(schema, str(output_path))

            elif export_format == "RDF/Turtle":
                exporter = RDFExporter()
                output_path = batch_dir / f"{clean_name}.ttl"
                exporter.export(schema, str(output_path), format="turtle")

            elif export_format == "SQL":
                exporter = SQLExporter()
                output_path = batch_dir / f"{clean_name}.sql"
                exporter.export(schema, str(output_path))

            else:
                return "‚ùå Format d'export invalide", None

            exported_count += 1

        message = f"‚úÖ Export {export_format} r√©ussi\n\n{exported_count} fichiers export√©s dans:\nüìÅ {batch_dir}"

        # Cr√©er un fichier zip pour faciliter le t√©l√©chargement
        zip_path = str(batch_dir) + ".zip"
        shutil.make_archive(str(batch_dir), 'zip', batch_dir)

        return message, zip_path

    except Exception as e:
        return f"‚ùå Erreur lors de l'export: {str(e)}", None


def _format_results_by_hub(hubs: list, links: list, satellites: list) -> str:
    """
    Formate les r√©sultats en structure Data Vault lin√©aire.

    Args:
        hubs: Liste des Hubs
        links: Liste des Links
        satellites: Liste des Satellites

    Returns:
        String format√©
    """
    if not hubs:
        return "Aucune entit√© d√©tect√©e"

    output = "## Structure Data Vault\n\n"
    output += "```\n"

    # Identifier le hub plante et le hub probl√®me
    hub_plante = None
    hub_probleme = None

    for hub in hubs:
        if hub.entity_type == 'plante':
            hub_plante = hub
        elif hub.entity_type in ('maladie', 'ravageur'):
            hub_probleme = hub

    # Afficher le Hub plante
    if hub_plante:
        output += f"HUB plante: {hub_plante.business_key}\n\n"

    # Afficher le Link et le Hub probl√®me
    if links:
        link = links[0]
        output += f"   LINK relation: {link.relation_type}\n\n"

    if hub_probleme:
        output += f"   HUB {hub_probleme.entity_type}: {hub_probleme.business_key}\n\n"

    # Afficher tous les Satellites
    for sat in satellites:
        # V√©rifier si c'est un match par similarit√© (score < 1.0)
        if sat.confidence_score < 1.0:
            output += f"   SATELLITE {sat.attribute_name} (score: {sat.confidence_score:.2f}): {sat.attribute_value}\n"
        else:
            output += f"   SATELLITE {sat.attribute_name}: {sat.attribute_value}\n"

    # Afficher le r√©sum√© du Link cr√©√©
    if hub_plante and hub_probleme and links:
        link = links[0]
        output += f"\n   LINK cr√©√©: {hub_plante.business_key} --{link.relation_type}--> {hub_probleme.business_key}\n"

    output += "```\n"

    return output


def export_schema(export_format: str) -> Tuple[str, Optional[str]]:
    """
    Exporte le dernier sch√©ma g√©n√©r√©.

    Args:
        export_format: Format d'export ("JSON", "RDF", "SQL")

    Returns:
        Tuple (message, chemin_fichier)
    """
    global last_schema

    if last_schema is None:
        return "‚ùå Aucun sch√©ma √† exporter. Analysez d'abord une image.", None

    try:
        # Cr√©er le dossier exports
        export_dir = Path(config.EXPORT_PATH)
        export_dir.mkdir(parents=True, exist_ok=True)

        timestamp = time.strftime("%Y%m%d_%H%M%S")

        if export_format == "JSON":
            exporter = JSONExporter()
            output_path = export_dir / f"schema_{timestamp}.json"
            exporter.export(last_schema, str(output_path))
            message = f"‚úÖ Export JSON r√©ussi"

        elif export_format == "RDF/Turtle":
            exporter = RDFExporter()
            output_path = export_dir / f"schema_{timestamp}.ttl"
            exporter.export(last_schema, str(output_path), format="turtle")
            message = f"‚úÖ Export RDF/Turtle r√©ussi"

        elif export_format == "SQL":
            exporter = SQLExporter()
            output_path = export_dir / f"schema_{timestamp}.sql"
            exporter.export(last_schema, str(output_path))
            message = f"‚úÖ Export SQL r√©ussi"

        else:
            return "‚ùå Format d'export invalide", None

        return f"{message}\nüìÅ Fichier: {output_path}", str(output_path)

    except Exception as e:
        return f"‚ùå Erreur lors de l'export: {str(e)}", None


def get_ontology_info() -> str:
    """Retourne les informations sur l'ontologie."""
    stats = ontology.get_statistics()

    info = f"""
# Ontologie - Plantes Agricoles du Burkina Faso

### Statistiques

- Triplets RDF: {stats['total_triples']}
- Concepts: {stats['concepts']} (Classes: {stats['classes']}, Instances: {stats['individuals']})
- Relations: {stats['relations']}
- Attributs: {stats['attributes']}

### Domaine

**Plantes couvertes:**
Ma√Øs, Oignon, Tomate

**√âl√©ments mod√©lis√©s:**
- Maladies (fongiques, bact√©riennes, virales, ravageurs, stress)
- Sympt√¥mes (n√©crose, chlorose, fl√©trissement, taches foliaires)
- Caract√©ristiques morphologiques (couleur, forme, texture, nervation)

### Fichier

`{config.ONTOLOGY_PATH}`
"""
    return info


# ========== CONSTRUCTION DE L'INTERFACE GRADIO ==========

# Th√®me personnalis√© sobre et moderne
theme = gr.themes.Monochrome(
    primary_hue="slate",
    secondary_hue="zinc",
    neutral_hue="stone",
    radius_size="sm",
    font=[gr.themes.GoogleFont("Inter"), "ui-sans-serif", "system-ui", "sans-serif"],
).set(
    body_background_fill="#fafafa",
    body_background_fill_dark="#0f0f0f",
    block_background_fill="white",
    block_border_width="1px",
    block_label_text_weight="500",
    button_primary_background_fill="#18181b",
    button_primary_background_fill_hover="#27272a",
)

with gr.Blocks(title="Extraction de Connaissances - Plantes", theme=theme, css="""
    .gradio-container {max-width: 1400px !important;}
    h1 {font-size: 1.75rem !important; font-weight: 600 !important; margin-bottom: 0.5rem !important;}
    h2 {font-size: 1.25rem !important; font-weight: 500 !important;}
    h3 {font-size: 1.1rem !important; font-weight: 500 !important; color: #52525b !important;}
    .prose {color: #3f3f46 !important;}
""") as app:
    gr.Markdown("""
    # Extraction de Connaissances
    ### Analyse automatique d'images de plantes agricoles

    Ma√Øs, Oignon, Tomate
    """)

    # ========== ONGLET 1: ANALYSE ==========
    with gr.Tab("Analyse"):
        with gr.Row():
            with gr.Column(scale=1):
                image_input = gr.Image(type="filepath", label="Image", height=300)

                with gr.Accordion("Configuration", open=False):
                    model_selector = gr.Dropdown(
                        choices=["llava:7b", "llava:13b", "qwen3-vl:latest", "llama3.2-vision:latest"],
                        value="llava:7b",
                        label="Mod√®le",
                        info="S√©lectionner le mod√®le"
                    )

                    # Liste des ontologies disponibles
                    available_ontologies = config.get_available_ontologies()
                    ontology_choices = list(available_ontologies.keys())
                    default_ontology = Path(config.ONTOLOGY_PATH).name

                    ontology_selector = gr.Dropdown(
                        choices=ontology_choices,
                        value=default_ontology if default_ontology in ontology_choices else (ontology_choices[0] if ontology_choices else None),
                        label="Ontologie",
                        info="S√©lectionner le fichier d'ontologie"
                    )

                    similarity_selector = gr.Dropdown(
                        choices=[
                            ("Hybride (Jaro-Winkler + Embeddings)", "hybrid"),
                            ("Jaro-Winkler + Cosinus", "jaro_cosine"),
                            ("Lexical (Jaro-Winkler)", "lexical"),
                            ("S√©mantique (Embeddings)", "semantic"),
                            ("Cosinus (N-grams)", "cosine"),
                            ("Jaro-Winkler", "jaro_winkler")
                        ],
                        value=config.SIMILARITY_ALGORITHM,
                        label="Mesure de similarit√©",
                        info="Algorithme de comparaison des lemmes"
                    )

                    threshold_e = gr.Slider(
                        0.0, 1.0, config.THRESHOLD_ENTITIES, step=0.05,
                        label="Seuil hub (entit√©s)"
                    )
                    threshold_r = gr.Slider(
                        0.0, 1.0, config.THRESHOLD_RELATIONS, step=0.05,
                        label="Seuil link (relations)"
                    )
                    threshold_a = gr.Slider(
                        0.0, 1.0, config.THRESHOLD_ATTRIBUTES, step=0.05,
                        label="Seuil satellite (attributs)"
                    )

                analyze_btn = gr.Button("Analyser", variant="primary", size="lg")

            with gr.Column(scale=2):
                status_box = gr.Markdown(label="Statut", value="")
                lemmas_output = gr.Markdown()
                stats_output = gr.Markdown()

        with gr.Row():
            results_by_hub = gr.Markdown()

        # Effacer les sorties puis lancer l'analyse
        analyze_btn.click(
            fn=clear_outputs,
            inputs=None,
            outputs=[status_box, lemmas_output, results_by_hub, stats_output]
        ).then(
            fn=analyze_image,
            inputs=[image_input, model_selector, ontology_selector, similarity_selector, threshold_e, threshold_r, threshold_a],
            outputs=[status_box, lemmas_output, results_by_hub, stats_output],
            show_progress="full"
        )

    # ========== ONGLET 2: TRAITEMENT PAR LOT ==========
    with gr.Tab("Traitement par lot"):
        with gr.Row():
            with gr.Column(scale=1):
                multiple_images_input = gr.File(
                    label="S√©lectionner plusieurs images",
                    file_count="multiple",
                    file_types=["image"],
                    type="filepath"
                )

                with gr.Accordion("Configuration", open=False):
                    batch_model_selector = gr.Dropdown(
                        choices=["llava:7b", "llava:13b", "qwen3-vl:latest", "llama3.2-vision:latest"],
                        value="llava:7b",
                        label="Mod√®le",
                        info="S√©lectionner le mod√®le"
                    )

                    # Liste des ontologies disponibles
                    available_ontologies = config.get_available_ontologies()
                    ontology_choices = list(available_ontologies.keys())
                    default_ontology = Path(config.ONTOLOGY_PATH).name

                    batch_ontology_selector = gr.Dropdown(
                        choices=ontology_choices,
                        value=default_ontology if default_ontology in ontology_choices else (ontology_choices[0] if ontology_choices else None),
                        label="Ontologie",
                        info="S√©lectionner le fichier d'ontologie"
                    )

                    batch_similarity_selector = gr.Dropdown(
                        choices=[
                            ("Hybride (Jaro-Winkler + Embeddings)", "hybrid"),
                            ("Jaro-Winkler + Cosinus", "jaro_cosine"),
                            ("Lexical (Jaro-Winkler)", "lexical"),
                            ("S√©mantique (Embeddings)", "semantic"),
                            ("Cosinus (N-grams)", "cosine"),
                            ("Jaro-Winkler", "jaro_winkler")
                        ],
                        value=config.SIMILARITY_ALGORITHM,
                        label="Mesure de similarit√©",
                        info="Algorithme de comparaison des lemmes"
                    )

                    batch_threshold_e = gr.Slider(
                        0.0, 1.0, config.THRESHOLD_ENTITIES, step=0.05,
                        label="Seuil hub (entit√©s)"
                    )
                    batch_threshold_r = gr.Slider(
                        0.0, 1.0, config.THRESHOLD_RELATIONS, step=0.05,
                        label="Seuil link (relations)"
                    )
                    batch_threshold_a = gr.Slider(
                        0.0, 1.0, config.THRESHOLD_ATTRIBUTES, step=0.05,
                        label="Seuil satellite (attributs)"
                    )

                process_btn = gr.Button("Traiter les images", variant="primary", size="lg")

                gr.Markdown("""
                ### Instructions
                1. Cliquez sur "S√©lectionner plusieurs images"
                2. S√©lectionnez toutes les images √† traiter (Ctrl+Clic ou Shift+Clic)
                3. Cliquez sur "Traiter les images"

                ### Formats support√©s
                JPG, JPEG, PNG, BMP, GIF, TIFF

                ### Note
                Le traitement peut prendre plusieurs minutes selon le nombre d'images.
                """)

            with gr.Column(scale=2):
                batch_status_box = gr.Markdown(label="Statut", value="")
                batch_results_output = gr.Markdown()

        # Action du bouton de traitement
        process_btn.click(
            fn=process_multiple_images,
            inputs=[
                multiple_images_input,
                batch_model_selector,
                batch_ontology_selector,
                batch_similarity_selector,
                batch_threshold_e,
                batch_threshold_r,
                batch_threshold_a
            ],
            outputs=[batch_status_box, batch_results_output],
            show_progress="full"
        )

        # Section Export des r√©sultats batch
        gr.Markdown("---")
        gr.Markdown("## Export des r√©sultats")

        with gr.Row():
            with gr.Column(scale=1):
                batch_export_format = gr.Radio(
                    choices=["JSON", "RDF/Turtle", "SQL"],
                    value="JSON",
                    label="Format"
                )
                batch_export_btn = gr.Button("Exporter tous les r√©sultats", variant="primary")

            with gr.Column(scale=2):
                batch_export_message = gr.Textbox(label="Statut", lines=3, show_label=False)
                batch_export_file = gr.File(label="T√©l√©chargement (ZIP)")

        batch_export_btn.click(
            fn=export_batch_results,
            inputs=[batch_export_format],
            outputs=[batch_export_message, batch_export_file]
        )

    # ========== ONGLET 3: EXPORT (Image unique) ==========
    with gr.Tab("Export"):
        with gr.Row():
            with gr.Column(scale=1):
                export_format = gr.Radio(
                    choices=["JSON", "RDF/Turtle", "SQL"],
                    value="JSON",
                    label="Format"
                )
                export_btn = gr.Button("Exporter", variant="primary")

            with gr.Column(scale=2):
                export_message = gr.Textbox(label="Statut", lines=2, show_label=False)
                export_file = gr.File(label="T√©l√©chargement")

        export_btn.click(
            fn=export_schema,
            inputs=[export_format],
            outputs=[export_message, export_file]
        )

    # ========== ONGLET 4: ONTOLOGIE ==========
    with gr.Tab("Ontologie"):
        ontology_info = gr.Markdown(value=get_ontology_info())

    # ========== ONGLET 5: INFORMATIONS ==========
    with gr.Tab("Info"):
        with gr.Row():
            with gr.Column():
                gr.Markdown("""
                ### Pipeline

                1. Extraction de lemmes (mod√®le vision)
                2. Classification ontologique
                3. Calcul de similarit√©
                4. G√©n√©ration sch√©ma Data Vault

                ### Technologies

                - **LLM**: Ollama
                - **Ontologie**: RDF/OWL
                - **Similarit√©**: Jaro-Winkler + embeddings
                - **Interface**: Gradio
                - **Data Vault**: Hubs, Links, Satellites
                """)

            with gr.Column():
                gr.Markdown(f"""
                ### Configuration

                **Mod√®les disponibles:**
                - llava:7b (4-5 GB)
                - qwen2.5vl:3b (10 GB)
                - qwen2.5vl:7b (12 GB)

                **Param√®tres:**
                - URL: `{config.OLLAMA_BASE_URL}`
                - Mod√®le: `{config.LLAVA_MODEL}`
                - Embeddings: `{config.EMBEDDING_MODEL}`

                **Seuils par d√©faut:**
                - Entit√©s: {config.THRESHOLD_ENTITIES}
                - Relations: {config.THRESHOLD_RELATIONS}
                - Attributs: {config.THRESHOLD_ATTRIBUTES}

                Version 1.0.0
                """)

# ========== LANCEMENT DE L'APPLICATION ==========

if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("üöÄ Lancement de l'interface Gradio")
    print("=" * 60)

    app.queue().launch(
        server_name=config.GRADIO_SERVER_NAME,
        server_port=config.GRADIO_SERVER_PORT,
        share=config.GRADIO_SHARE,
        theme=theme
    )
